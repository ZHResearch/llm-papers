# For documentation on this file, see:
# https://github.com/riggraz/no-style-please#customize-the-menu

entries:
  - title: "<h2>20230822</h2>"
  - title: "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction [Apr 2023]"
    url: https://arxiv.org/abs/2304.08085
    entries:
      - title: "Experimental results demonstrate that the proposed InstructUIE method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings."
  - title: "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks [May 2023]"
    url: https://arxiv.org/abs/2305.11430
    entries:
      - title: "A general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks is proposed that will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies."
  - title: "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning [ACL 2023]"
    url: https://aclanthology.org/2023.acl-long.172/
    entries:
      - title: "This paper systematically study the role of task definitions in instruction learning, and proposes two strategies to help models better leverage task instructions: providing only key information for tasks in a common structured format, and adding a meta-tuning stage to help the model better understand the definitions."
  - title: PPT made by Xu
    url: /ppt/xs_20230822.pdf
  - title: "<h2>20230808</h2>"
  - title: "Finetuned Language Models Are Zero-Shot Learners [Sep 2021]"
    url: https://arxiv.org/abs/2109.01652
    entries:
      - title: "It is shown that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin."
  - title: "Self-Instruct: Aligning Language Models with Self-Generated Instructions [ACL 2023]"
    url: https://aclanthology.org/2023.acl-long.754/
    entries:
      - title: "Self-Instruct is introduced, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations by generating instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model."
  - title: "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning [May 2023]"
    url: https://arxiv.org/abs/2305.09246
    entries:
      - title: "A preliminary exploration into reducing the data used in LLM instruction tuning and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of data required for task-specific models."
  - title: "Specializing Smaller Language Models towards Multi-Step Reasoning [Jan 2023]"
    url: https://arxiv.org/abs/2301.12726
    entries:
      - title: "This work shows two important aspects of model abilities: there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities, and by paying the price of decreased generic ability, it can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability."
  - title: "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4 [May 2023]"
    url: https://arxiv.org/abs/2305.12147
    entries:
      - title: "LogiCoT is presented, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4 that serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills."
  - title: "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning [Aug 2023]"
    url: https://arxiv.org/abs/2308.00436
    entries:
      - title: "This work proposes a zero-shot verification scheme to recognize individual errors within a step-by-step reasoning in large language models and uses it to improve question-answering performance, by using it to perform weighted voting on different generated answers."
  - title: "Chinese Open Instruction Generalist: A Preliminary Release [Apr 2023]"
    url: https://arxiv.org/abs/2304.07987
    entries:
      - title: "This work proposes the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks by various Methods, collecting around 200k Chinese instruction tuning samples."
  - title: PPT made by Xu
    url: /ppt/xs_20230808.pdf
  - title: "<h2>20230725</h2>"
  - title: "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond [Apr 2023]"
    url: https://arxiv.org/abs/2304.13712
    entries:
      - title: "A comprehensive and practical guide for practitioners and end-users working with Large Language Models in their downstream natural language processing (NLP) tasks, enabling the successful implementation of these models in a wide range of NLP tasks."
  - title: "A Survey of Large Language Models [Mar 2023]"
    url: https://arxiv.org/abs/2303.18223
    entries:
      - title: "A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation."
  - title: "Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning [Mar 2023]"
    url: https://arxiv.org/abs/2303.10475
    entries:
      - title: "This survey paper tries to summarize and provide insights into the current research on instruction learning, particularly by answering the following questions: What is task instruction, and what instruction types exist?"
  - title: PPT made by Xu
    url: /ppt/xs_20230725.pdf
  - title: "<h2>20230531</h2>"
  - title: "Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors [May 2023]"
    url: https://arxiv.org/abs/2305.14450
    entries:
      - title: "This paper evaluates ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and finds a huge performance gap between ChatG PT and SOTA results, and proposes a soft-matching strategy for evaluation."
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_3.pdf
  - title: "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities [May 2023]"
    url: https://arxiv.org/abs/2305.13168
    entries:
      - title: An exhaustive quantitative and qualitative evaluation of Large Language Models for Knowledge Graph (KG) construction and reasoning, which suggests that GPT-4 outperforms ChatGPT in the majority of tasks and even surpasses fine-tuned models in certain reasoning and question-answering datasets.
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_3.pdf
  - title: "SummIt: Iterative Text Summarization via ChatGPT [May 2023]"
    url: https://arxiv.org/abs/2305.14835
    entries:
      - title: This paper proposes SummIt, an iterative text summarization framework based on large language models like ChatGPT that enables the model to refine the generated summary iteratively through self-evaluation and feedback, closely resembling the iterative process humans undertake when drafting and revising summaries.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.31.pdf
  - title: "ClusterLLM: Large Language Models as a Guide for Text Clustering [May 2023]"
    url: https://arxiv.org/abs/2305.14871
    entries:
      - title: ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT, consistently improves clustering quality, at an average cost of ~$0.6 per dataset.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.31.pdf
  - title: "<h2>20230517</h2>"
  - title: "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness [Apr 2023]"
    url: https://arxiv.org/abs/2304.11633
    entries:
      - title: "ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation, and indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions."
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_2.pdf
  - title: "ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs [May 2023]"
    url: https://arxiv.org/abs/2305.03513
    entries:
      - title: A novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability and providing a more transparent decision-making process compared with previous text classification methods is proposed.
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_2.pdf
  - title: "VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna [May 2023]"
    url: https://arxiv.org/abs/2305.03253
    entries:
      - title: VicunaNER is a two-phase framework, where each phase leverages multi-turn dialogues with Vicuna to recognize entities from texts and names the second phase as Re-Recognition, which recognizes those entities not recognized in the first phase.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.17.pdf
  - title: Using ChatGPT for Entity Matching [May 2023]
    url: https://arxiv.org/abs/2305.03423
    entries:
      - title: This paper investigates using ChatGPT for entity matching as a more robust, training data-efficient alternative to traditional Transformer models, and shows that ChatGPT is competitive with a fine-tuned RoBERTa model, reaching an average zero-shot performance of 83% F1 on a challenging matching task.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.17.pdf
  - title: ChatGPT as a Text Simplification Tool to Remove Bias [May 2023]
    url: https://arxiv.org/abs/2305.06166
    entries:
      - title: A possible technique for bias mitigation in the form of simplification of text is explored, which is that simplifying text should standardise language to one way of speaking while keeping the same meaning.
      - title: PPT made by Liu
        url: /ppt/liu_20230516.pdf
  - title: "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors [May 2023]"
    url: https://arxiv.org/abs/2305.05711
    entries:
      - title: This paper proposes to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction.
      - title: PPT made by Liu
        url: /ppt/liu_20230516.pdf
  - title: Is ChatGPT Equipped with Emotional Dialogue Capabilities? [Apr 2023]
    url: https://arxiv.org/abs/2304.09582
    entries:
      - title: This study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks.
      - title: PPT made by Sun
        url: /ppt/sun_sd2.pdf
  - title: How would Stance Detection Techniques Evolve after the Launch of ChatGPT? [Apr 2023]
    url: https://arxiv.org/abs/2212.14548
    entries:
      - title: For the stance detection tasks, experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance, and can provide explanation for its own prediction, which is beyond the capability of any existing model.
      - title: PPT made by Sun
        url: /ppt/sun_sd2.pdf
  - title: "<h2>20230503</h2>"
  - title: Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study [Apr 2023]
    url: https://arxiv.org/abs/2304.04339
    entries:
      - title: A preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text and compares it with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task.
      - title: PPT made by Liu
        url: /ppt/liu_20230503.pdf
  - title: Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media [Apr 2023]
    url: https://arxiv.org/abs/2304.03087
    entries:
      - title: "This paper examines CoT's effectiveness in stance detection tasks, demonstrating its superior accuracy and discussing associated challenges."
      - title: PPT made by Liu
        url: /ppt/liu_20230503.pdf
  - title: "ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT [Apr 2023]"
    url: https://arxiv.org/abs/2304.14334
    entries:
      - title: It is shown that with appropriate task-specific ChatGPT prompts, the use of data obtained from prompting a large generative language model,ChatGPT, to generate synthetic training data with the aim of augmenting data in low resource scenarios outperform the most popular existing approaches for such data augmentation.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.03.pdf
  - title: "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models [Mar 2023]"
    url: https://arxiv.org/abs/2303.04671
    entries:
      - title: A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.03.pdf
  - title: Human-like Summarization Evaluation with ChatGPT [Apr 2023]
    url: https://arxiv.org/abs/2304.02554
    entries:
      - title: ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation, and it outperformed commonly used automatic evaluation metrics on some datasets.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt5.3.pdf
  - title: Linguistic ambiguity analysis in ChatGPT [Feb 2023]
    url: https://arxiv.org/abs/2302.06426
    entries:
      - title: An introduction to linguistic ambiguity, its varieties and their relevance in modern NLP, and an extensive empiric analysis are provided, as well as strategies to get the most of this model.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt5.3.pdf
  - title: "<h2>20230419</h2>"
  - title: Zero-Shot Information Extraction via Chatting with ChatGPT [Feb 2023]
    url: https://arxiv.org/abs/2302.10205
    entries:
      - title: "This work transforms the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE), and extensively evaluates the framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction."
      - title: PPT made by Liu
        url: /ppt/liu_20230418.pdf
  - title: "AugGPT: Leveraging ChatGPT for Text Data Augmentation [Feb 2023]"
    url: https://arxiv.org/abs/2302.13007
    entries:
      - title: Experimental results on few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.
      - title: PPT made by Liu
        url: /ppt/liu_20230418.pdf
  - title: Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine [Jan 2023]
    url: https://arxiv.org/abs/2301.08745
    entries:
      - title: A preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness finds that it performs competitively with commercial translation products on high-resource European languages but lags behind significantly on low-resource or distant languages.
      - title: PPT made by Sun
        url: /
  - title: How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection [Jan 2023]
    url: https://arxiv.org/abs/2301.07597
    entries:
      - title: This work collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas, and builds three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios.
      - title: PPT made by Sun
        url: /
  - title: "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research [Mar 2023]"
    url: https://arxiv.org/abs/2303.17395
    entries:
      - title: This work introduces WavCaps, the first large-scale weakly-labelled audio captioning dataset, and proposes a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt4.19.pdf
  - title: "Exploring the Feasibility of ChatGPT for Event Extraction [Mar 2023]"
    url: https://arxiv.org/abs/2303.03836
    entries:
      - title: The usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt4.19.pdf
  - title: "Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses [Apr 2023]"
    url: https://arxiv.org/abs/2304.05774
    entries:
      - title: A research prototype, called GPToLODS, is presented, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs, and identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.04.19.pdf
  - title: "Zero-shot Temporal Relation Extraction with ChatGPT [Apr 2023]"
    url: https://arxiv.org/abs/2304.05454
    entries:
      - title: It is found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.04.19.pdf
  - title: Extractive Summarization via ChatGPT for Faithful Summary Generation [Apr 2023]
    url: https://arxiv.org/abs/2304.04193
    entries:
      - title: It is found that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness, and highlights potential directions for enhancing ChatG PT's capabilities for faithful text summarization tasks using two-stage approaches.
      - title: PPT made by Xu
        url: /ppt/xu_chatgpt_1.pdf
  - title: Zero-shot Clinical Entity Recognition using ChatGPT [Mar 2023]
    url: https://arxiv.org/abs/2303.16416
    entries:
      - title: This study investigated the potential of ChatGPT for the clinical named entity recognition task in a zero-shot setting with two different prompt strategies.
      - title: PPT made by Xu
        url: /ppt/xu_chatgpt_1.pdf

  