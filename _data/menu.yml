# For documentation on this file, see:
# https://github.com/riggraz/no-style-please#customize-the-menu

entries:
  - title: "<h2>20231122</h2>"
  - title: "Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction [Nov 2023]"
    url: https://arxiv.org/abs/2311.06555
    entries:
      - title: The Heuristic-Driven Link-of-Analogy (HD-LoA) prompting method is introduced, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their adaptability and inspired by the analogical reasoning of human.
  - title: "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning [Nov 2023]"
    url: https://arxiv.org/abs/2311.03734
    entries:
      - title: This work investigates constructing and leveraging extracted semantic structures (graphs) for multi-hop question answering, especially the reasoning process, and generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets.
      - title: PPT made by XuSheng
        url: /ppt/xs_20231122.pdf
  - title: "ChatHaruhi: Reviving Anime Character in Reality via Large Language Model [Aug 2023]"
    url: https://arxiv.org/abs/2308.09597
    entries:
      - title: Both automatic and human evaluations show the approach improves role-playing ability over baselines, and an algorithm that controls language models via an improved prompt and memories of the character extracted from scripts is proposed.
  - title: "Adapting Fake News Detection to the Era of Large Language Models [Nov 2023]"
    url: https://arxiv.org/abs/2311.04917
    entries:
      - title: A comprehensive evaluation of fake news detectors trained in various scenarios reveals an interesting pattern that detectors trained exclusively on human-written articles can indeed perform well at detecting machine-generated fake news, but not vice versa.
      - title: PPT made by SunTiening
        url: /ppt/sun_sd4.pdf
  - title: "<h2>20231108</h2>"
  - title: "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks [Oct 2023]"
    url: https://arxiv.org/abs/2310.14623
    entries:
      - title: This work proposes Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities.
      - title: PPT made by XuSheng
        url: /ppt/xs_20231108.pdf
  - title: "Learning From Mistakes Makes LLM Better Reasoner [Oct 2023]"
    url: https://arxiv.org/abs/2310.20689
    entries:
      - title: LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4 and improves the performance compared with fine-tuning on CoT data alone, surpassing the SOTA performance achieved by non-execution open-source models on these challenging tasks.
      - title: PPT made by XuSheng
        url: /ppt/xs_20231108.pdf
  - title: "Simple synthetic data reduces sycophancy in large language models [Aug 2023]"
    url: https://arxiv.org/abs/2308.03958
    entries:
      - title: A straightforward synthetic-data intervention is presented that takes public NLP tasks and encourages models to be robust to user opinions on these tasks and can significantly reduce sycophantic behavior on held-out prompts.
      - title: PPT made by LiuChengwei
        url: /liu_202311108.pdf
  - title: "Why Does ChatGPT Fall Short in Providing Truthful Answers? [Apr 2023]"
    url: https://arxiv.org/abs/2304.10513
    entries:
      - title: Analysis of the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures indicates that furnishing the model with fine-grained external knowledge, hints for knowledge recall, and guidance for reasoning can empower the model to answer questions more truthfully.
      - title: PPT made by LiuChengwei
        url: /liu_202311108.pdf
  - title: "Visual Instruction Tuning [Apr 2023]"
    url: https://arxiv.org/abs/2304.08485
    entries:
      - title: "This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available."
      - title: PPT made by SunTiening
        url: /ppt/sun_sd3.pdf
  - title: "Are aligned neural networks adversarially aligned? [Jun 2023]"
    url: https://arxiv.org/abs/2306.15447
    entries:
      - title: It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.
      - title: PPT made by SunTiening
        url: /ppt/sun_sd3.pdf
  - title: "STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models [May 2023]"
    url: https://arxiv.org/abs/2305.15090
    entries:
      - title: STARS is a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance and surpassing the effectiveness of human-curated data.
      - title: PPT made by WangYongsheng
        url: /ppt/wys_gpt20231107.pdf
  - title: "Empirical Study of Zero-Shot NER with ChatGPT [Oct 2023]"
    url: https://arxiv.org/abs/2310.10035
    entries:
      - title: This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task, and proposes syntactic augmentation to stimulate the model's intermediate thinking in two ways.
      - title: PPT made by WangYongsheng
        url: /ppt/wys_gpt20231107.pdf
  - title: "TarGEN: Targeted Data Generation with Large Language Models [Oct 2023]"
    url: https://arxiv.org/abs/2310.17876
    entries:
      - title: TarGEN is presented, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM that augments TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels.
      - title: PPT made by ChenXinyu
        url: /ppt/cxy_2023.11.08.pdf
  - title: "Making Large Language Models Better Data Creators [Oct 2023]"
    url: https://arxiv.org/abs/2310.20111
    entries:
      - title: This paper proposes a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces, and demonstrates that instruction-following LLMs are highly cost-effective data creators.
      - title: PPT made by ChenXinyu
        url: /ppt/cxy_2023.11.08.pdf
  - title: "Kosmos-2: Grounding Multimodal Large Language Models to the World [Jun 2023]"
    url: https://arxiv.org/abs/2306.14824
    entries:
      - title: Kosmos-2, a Multimodal Large Language Model (MLLM), is introduced, enabling new capabilities of perceiving object descriptions and grounding text to the visual world and sheds light on the big convergence of language, multimodal perception, action, and world modeling.
      - title: PPT made by ChenLizhi
        url: /ppt/clz_23.11.8.pdf
  - title: "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model [Jun 2023]"
    url: https://arxiv.org/abs/2306.09085
    entries:
      - title: COSA, a COncatenated SAmple pretrained vision-language foundation model, jointly models visual contents and event-level temporal cues using only image-text corpora and achieves state-of-the-art results on various competitive benchmarks.
      - title: PPT made by ChenLizhi
        url: /ppt/clz_23.11.8.pdf
  - title: "ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations [Apr 2023]"
    url: https://arxiv.org/abs/2304.14827
    entries:
      - title: It is found that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order between two events.
      - title: PPT made by LiuShannan
        url: /ppt/lsn_20231108.pdf
  - title: "Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study [May 2023]"
    url: https://arxiv.org/abs/2305.08391
    entries:
      - title: The results show that the generative paradigm allows ChatGPT to achieve comparative performance in the topic segmentation task comparable to state-of-the-art methods but reveals room for improvement in the more complex tasks of discourse relation recognition and discourse parsing.
      - title: PPT made by LiuShannan
        url: /ppt/lsn_20231108.pdf
  - title: "<h2>20230822</h2>"
  - title: "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction [Apr 2023]"
    url: https://arxiv.org/abs/2304.08085
    entries:
      - title: "Experimental results demonstrate that the proposed InstructUIE method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings."
  - title: "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks [May 2023]"
    url: https://arxiv.org/abs/2305.11430
    entries:
      - title: "A general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks is proposed that will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies."
  - title: "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning [ACL 2023]"
    url: https://aclanthology.org/2023.acl-long.172/
    entries:
      - title: "This paper systematically study the role of task definitions in instruction learning, and proposes two strategies to help models better leverage task instructions: providing only key information for tasks in a common structured format, and adding a meta-tuning stage to help the model better understand the definitions."
  - title: PPT made by Xu
    url: /ppt/xs_20230822.pdf
  - title: "<h2>20230808</h2>"
  - title: "Finetuned Language Models Are Zero-Shot Learners [Sep 2021]"
    url: https://arxiv.org/abs/2109.01652
    entries:
      - title: "It is shown that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin."
  - title: "Self-Instruct: Aligning Language Models with Self-Generated Instructions [ACL 2023]"
    url: https://aclanthology.org/2023.acl-long.754/
    entries:
      - title: "Self-Instruct is introduced, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations by generating instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model."
  - title: "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning [May 2023]"
    url: https://arxiv.org/abs/2305.09246
    entries:
      - title: "A preliminary exploration into reducing the data used in LLM instruction tuning and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of data required for task-specific models."
  - title: "Specializing Smaller Language Models towards Multi-Step Reasoning [Jan 2023]"
    url: https://arxiv.org/abs/2301.12726
    entries:
      - title: "This work shows two important aspects of model abilities: there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities, and by paying the price of decreased generic ability, it can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability."
  - title: "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4 [May 2023]"
    url: https://arxiv.org/abs/2305.12147
    entries:
      - title: "LogiCoT is presented, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4 that serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills."
  - title: "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning [Aug 2023]"
    url: https://arxiv.org/abs/2308.00436
    entries:
      - title: "This work proposes a zero-shot verification scheme to recognize individual errors within a step-by-step reasoning in large language models and uses it to improve question-answering performance, by using it to perform weighted voting on different generated answers."
  - title: "Chinese Open Instruction Generalist: A Preliminary Release [Apr 2023]"
    url: https://arxiv.org/abs/2304.07987
    entries:
      - title: "This work proposes the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks by various Methods, collecting around 200k Chinese instruction tuning samples."
  - title: PPT made by Xu
    url: /ppt/xs_20230808.pdf
  - title: "<h2>20230725</h2>"
  - title: "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond [Apr 2023]"
    url: https://arxiv.org/abs/2304.13712
    entries:
      - title: "A comprehensive and practical guide for practitioners and end-users working with Large Language Models in their downstream natural language processing (NLP) tasks, enabling the successful implementation of these models in a wide range of NLP tasks."
  - title: "A Survey of Large Language Models [Mar 2023]"
    url: https://arxiv.org/abs/2303.18223
    entries:
      - title: "A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation."
  - title: "Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning [Mar 2023]"
    url: https://arxiv.org/abs/2303.10475
    entries:
      - title: "This survey paper tries to summarize and provide insights into the current research on instruction learning, particularly by answering the following questions: What is task instruction, and what instruction types exist?"
  - title: PPT made by Xu
    url: /ppt/xs_20230725.pdf
  - title: "<h2>20230531</h2>"
  - title: "Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors [May 2023]"
    url: https://arxiv.org/abs/2305.14450
    entries:
      - title: "This paper evaluates ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and finds a huge performance gap between ChatG PT and SOTA results, and proposes a soft-matching strategy for evaluation."
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_3.pdf
  - title: "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities [May 2023]"
    url: https://arxiv.org/abs/2305.13168
    entries:
      - title: An exhaustive quantitative and qualitative evaluation of Large Language Models for Knowledge Graph (KG) construction and reasoning, which suggests that GPT-4 outperforms ChatGPT in the majority of tasks and even surpasses fine-tuned models in certain reasoning and question-answering datasets.
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_3.pdf
  - title: "SummIt: Iterative Text Summarization via ChatGPT [May 2023]"
    url: https://arxiv.org/abs/2305.14835
    entries:
      - title: This paper proposes SummIt, an iterative text summarization framework based on large language models like ChatGPT that enables the model to refine the generated summary iteratively through self-evaluation and feedback, closely resembling the iterative process humans undertake when drafting and revising summaries.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.31.pdf
  - title: "ClusterLLM: Large Language Models as a Guide for Text Clustering [May 2023]"
    url: https://arxiv.org/abs/2305.14871
    entries:
      - title: ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT, consistently improves clustering quality, at an average cost of ~$0.6 per dataset.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.31.pdf
  - title: "<h2>20230517</h2>"
  - title: "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness [Apr 2023]"
    url: https://arxiv.org/abs/2304.11633
    entries:
      - title: "ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation, and indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions."
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_2.pdf
  - title: "ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs [May 2023]"
    url: https://arxiv.org/abs/2305.03513
    entries:
      - title: A novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability and providing a more transparent decision-making process compared with previous text classification methods is proposed.
      - title: PPT made by Xu
        url: /ppt/xs_chatgpt_2.pdf
  - title: "VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna [May 2023]"
    url: https://arxiv.org/abs/2305.03253
    entries:
      - title: VicunaNER is a two-phase framework, where each phase leverages multi-turn dialogues with Vicuna to recognize entities from texts and names the second phase as Re-Recognition, which recognizes those entities not recognized in the first phase.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.17.pdf
  - title: Using ChatGPT for Entity Matching [May 2023]
    url: https://arxiv.org/abs/2305.03423
    entries:
      - title: This paper investigates using ChatGPT for entity matching as a more robust, training data-efficient alternative to traditional Transformer models, and shows that ChatGPT is competitive with a fine-tuned RoBERTa model, reaching an average zero-shot performance of 83% F1 on a challenging matching task.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.17.pdf
  - title: ChatGPT as a Text Simplification Tool to Remove Bias [May 2023]
    url: https://arxiv.org/abs/2305.06166
    entries:
      - title: A possible technique for bias mitigation in the form of simplification of text is explored, which is that simplifying text should standardise language to one way of speaking while keeping the same meaning.
      - title: PPT made by Liu
        url: /ppt/liu_20230516.pdf
  - title: "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors [May 2023]"
    url: https://arxiv.org/abs/2305.05711
    entries:
      - title: This paper proposes to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction.
      - title: PPT made by Liu
        url: /ppt/liu_20230516.pdf
  - title: Is ChatGPT Equipped with Emotional Dialogue Capabilities? [Apr 2023]
    url: https://arxiv.org/abs/2304.09582
    entries:
      - title: This study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks.
      - title: PPT made by Sun
        url: /ppt/sun_sd2.pdf
  - title: How would Stance Detection Techniques Evolve after the Launch of ChatGPT? [Apr 2023]
    url: https://arxiv.org/abs/2212.14548
    entries:
      - title: For the stance detection tasks, experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance, and can provide explanation for its own prediction, which is beyond the capability of any existing model.
      - title: PPT made by Sun
        url: /ppt/sun_sd2.pdf
  - title: "<h2>20230503</h2>"
  - title: Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study [Apr 2023]
    url: https://arxiv.org/abs/2304.04339
    entries:
      - title: A preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text and compares it with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task.
      - title: PPT made by Liu
        url: /ppt/liu_20230503.pdf
  - title: Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media [Apr 2023]
    url: https://arxiv.org/abs/2304.03087
    entries:
      - title: "This paper examines CoT's effectiveness in stance detection tasks, demonstrating its superior accuracy and discussing associated challenges."
      - title: PPT made by Liu
        url: /ppt/liu_20230503.pdf
  - title: "ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT [Apr 2023]"
    url: https://arxiv.org/abs/2304.14334
    entries:
      - title: It is shown that with appropriate task-specific ChatGPT prompts, the use of data obtained from prompting a large generative language model,ChatGPT, to generate synthetic training data with the aim of augmenting data in low resource scenarios outperform the most popular existing approaches for such data augmentation.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.03.pdf
  - title: "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models [Mar 2023]"
    url: https://arxiv.org/abs/2303.04671
    entries:
      - title: A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.05.03.pdf
  - title: Human-like Summarization Evaluation with ChatGPT [Apr 2023]
    url: https://arxiv.org/abs/2304.02554
    entries:
      - title: ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation, and it outperformed commonly used automatic evaluation metrics on some datasets.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt5.3.pdf
  - title: Linguistic ambiguity analysis in ChatGPT [Feb 2023]
    url: https://arxiv.org/abs/2302.06426
    entries:
      - title: An introduction to linguistic ambiguity, its varieties and their relevance in modern NLP, and an extensive empiric analysis are provided, as well as strategies to get the most of this model.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt5.3.pdf
  - title: "<h2>20230419</h2>"
  - title: Zero-Shot Information Extraction via Chatting with ChatGPT [Feb 2023]
    url: https://arxiv.org/abs/2302.10205
    entries:
      - title: "This work transforms the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE), and extensively evaluates the framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction."
      - title: PPT made by Liu
        url: /ppt/liu_20230418.pdf
  - title: "AugGPT: Leveraging ChatGPT for Text Data Augmentation [Feb 2023]"
    url: https://arxiv.org/abs/2302.13007
    entries:
      - title: Experimental results on few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.
      - title: PPT made by Liu
        url: /ppt/liu_20230418.pdf
  - title: Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine [Jan 2023]
    url: https://arxiv.org/abs/2301.08745
    entries:
      - title: A preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness finds that it performs competitively with commercial translation products on high-resource European languages but lags behind significantly on low-resource or distant languages.
      - title: PPT made by Sun
        url: /
  - title: How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection [Jan 2023]
    url: https://arxiv.org/abs/2301.07597
    entries:
      - title: This work collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas, and builds three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios.
      - title: PPT made by Sun
        url: /
  - title: "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research [Mar 2023]"
    url: https://arxiv.org/abs/2303.17395
    entries:
      - title: This work introduces WavCaps, the first large-scale weakly-labelled audio captioning dataset, and proposes a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt4.19.pdf
  - title: "Exploring the Feasibility of ChatGPT for Event Extraction [Mar 2023]"
    url: https://arxiv.org/abs/2303.03836
    entries:
      - title: The usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience.
      - title: PPT made by Wang
        url: /ppt/wys_chatgpt4.19.pdf
  - title: "Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses [Apr 2023]"
    url: https://arxiv.org/abs/2304.05774
    entries:
      - title: A research prototype, called GPToLODS, is presented, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs, and identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.04.19.pdf
  - title: "Zero-shot Temporal Relation Extraction with ChatGPT [Apr 2023]"
    url: https://arxiv.org/abs/2304.05454
    entries:
      - title: It is found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.
      - title: PPT made by Chen
        url: /ppt/cxy_2023.04.19.pdf
  - title: Extractive Summarization via ChatGPT for Faithful Summary Generation [Apr 2023]
    url: https://arxiv.org/abs/2304.04193
    entries:
      - title: It is found that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness, and highlights potential directions for enhancing ChatG PT's capabilities for faithful text summarization tasks using two-stage approaches.
      - title: PPT made by Xu
        url: /ppt/xu_chatgpt_1.pdf
  - title: Zero-shot Clinical Entity Recognition using ChatGPT [Mar 2023]
    url: https://arxiv.org/abs/2303.16416
    entries:
      - title: This study investigated the potential of ChatGPT for the clinical named entity recognition task in a zero-shot setting with two different prompt strategies.
      - title: PPT made by Xu
        url: /ppt/xu_chatgpt_1.pdf

  